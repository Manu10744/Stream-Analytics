{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies and set constants\n",
    "from pyflink.datastream.functions import ProcessFunction\n",
    "from pyflink.common.serialization import SimpleStringSchema, SerializationSchema\n",
    "from pyflink.common.typeinfo import Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic\n",
    "from pyflink.datastream.connectors import FlinkKafkaConsumer, FlinkKafkaProducer\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import time\n",
    "import sys\n",
    "\n",
    "producer_props = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'queue.buffering.max.messages': '1000000'\n",
    "}\n",
    "\n",
    "consumer_props = {\n",
    "    \"bootstrap.servers\": \"localhost:9092\",\n",
    "    \"group.id\": \"twitter-consumers\",\n",
    "    \"client.id\": \"client-1\",\n",
    "}\n",
    "\n",
    "KAFKA_TOPIC = \"twitter-stream\"\n",
    "KAFKA_CONNECTOR_JAR = \"file:///home/ubuntu/Stream-Analytics/flink/flink-sql-connector-kafka_2.11-1.12.2.jar\"\n",
    "\n",
    "# Number of tweets in the dataset (10 MB)\n",
    "NUMBER_OF_TWEETS = 14485\n",
    "\n",
    "# Number of times the dataset is produced by Kafka\n",
    "NUMBER_OF_PRODUCTIONS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latencies = []\n",
    "# records_received = 0\n",
    "\n",
    "# def collect_stats():\n",
    "#     plt.clf()\n",
    "#     plt.plot(latencies)\n",
    "#     plt.savefig('latency_plot.png')\n",
    "\n",
    "\n",
    "class MyProcessFunction(ProcessFunction):\n",
    "    def process_element(self, value, ctx: 'ProcessFunction.Context'):\n",
    "        latency = (time.time() * 1000) - ctx.timestamp()\n",
    "        yield str(latency)\n",
    "        \n",
    "        # global might not be ideal here (parallelism possible?) but currently nothing else comes to mind\n",
    "#         global records_received\n",
    "#         records_received += 1\n",
    "#         latencies.append(latency)\n",
    "        \n",
    "#         if records_received == NUMBER_OF_TWEETS * NUMBER_OF_PRODUCTIONS:\n",
    "#             collect_stats()\n",
    "\n",
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "'''\n",
    "An execution environment defines a default parallelism for all operators, data sources,\n",
    "and data sinks it executes\n",
    "'''\n",
    "env.set_stream_time_characteristic(TimeCharacteristic.EventTime)\n",
    "\n",
    "# Add the Kafka Connector Dependency\n",
    "env.add_jars(KAFKA_CONNECTOR_JAR)\n",
    "env.add_classpaths(KAFKA_CONNECTOR_JAR)\n",
    "\n",
    "kafka_consumer = FlinkKafkaConsumer(\"twitter-stream\", SimpleStringSchema(), consumer_props)\n",
    "kafka_producer = FlinkKafkaProducer(\"twitter-stream-results\", SimpleStringSchema(), producer_props)\n",
    "\n",
    "stream = env.add_source(kafka_consumer)\n",
    "stream.process(MyProcessFunction(), output_type=Types.STRING()) \\\n",
    "      .add_sink(kafka_producer)\n",
    "\n",
    "env.execute_async()\n",
    "\n",
    "'''\n",
    "Weird stuff:\n",
    "    - Sometimes more records than emitted by Kafka (sometimes also less)\n",
    "    - print output sometimes was \"Latency: {}   Record-Number: {}   Total Number of Records: {}\".format(str(latency), str(len(latencies)), str(NUMBER_OF_TWEETS * NUMBER_OF_PRODUCTIONS))\n",
    "      and sometimes str(latency)!?!\n",
    "    - what value for parallelism?\n",
    "    \n",
    "Notes:\n",
    "    - Flink parallelism: https://stackoverflow.com/questions/61139187/what-is-the-difference-between-parallelism-and-parallel-computing-in-flink\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results by consuming the result stream from Kafka\n",
    "import platform, socket, json, psutil, logging, multiprocessing\n",
    "\n",
    "from time import time, perf_counter\n",
    "from confluent_kafka import Consumer, KafkaError\n",
    "\n",
    "# Create Kafka Consumer\n",
    "consumer_config = {\n",
    "    \"bootstrap.servers\": \"localhost:9092\",\n",
    "    \"group.id\": \"twitter-consumers\",\n",
    "    \"client.id\": \"client-1\",\n",
    "    \"enable.auto.commit\": True,\n",
    "    \"session.timeout.ms\": 6000,\n",
    "    \"default.topic.config\": {\"auto.offset.reset\": \"smallest\"}\n",
    "}\n",
    "c = Consumer(consumer_config)\n",
    "\n",
    "msg_counter = 0\n",
    "\n",
    "c.subscribe([\"twitter-stream-results\"])\n",
    "while True:             \n",
    "    msg = c.poll(0.5)\n",
    "\n",
    "    if msg is None:\n",
    "        continue\n",
    "    elif not msg.error():\n",
    "        msg_counter += 1\n",
    "        print(\"Message: {} Latency: {}\".format(msg_counter, msg.value()))\n",
    "#         msg_counter += 1\n",
    "        # Start the timer once the first message was received\n",
    "#         if msg_counter == 1:\n",
    "#             start = perf_counter()\n",
    "#         latencies.append(latency)\n",
    "    elif msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "        print(\"End of partition reached {}/{}\".format(msg.topic(), msg.partition()))\n",
    "        print(\"Messages consumed: {}\".format(msg_counter))\n",
    "    else:\n",
    "        print(\"Error occured: {}\".format(msg.error().str()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
