{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies and set constants\n",
    "from pyflink.datastream.functions import ProcessFunction\n",
    "from pyflink.common.serialization import SimpleStringSchema, SerializationSchema\n",
    "from pyflink.common.typeinfo import Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic\n",
    "from pyflink.datastream.connectors import FlinkKafkaConsumer, FlinkKafkaProducer\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import time\n",
    "import sys\n",
    "\n",
    "producer_props = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'queue.buffering.max.messages': '1000000'\n",
    "}\n",
    "\n",
    "consumer_props = {\n",
    "    \"bootstrap.servers\": \"localhost:9092\",\n",
    "    \"group.id\": \"twitter-consumers\",\n",
    "    \"client.id\": \"client-1\",\n",
    "}\n",
    "\n",
    "KAFKA_TOPIC = \"twitter-stream\"\n",
    "KAFKA_CONNECTOR_JAR = \"file:///home/ubuntu/flink-sql-connector-kafka_2.11-1.12.2.jar\"\n",
    "\n",
    "# Number of tweets in the dataset (10 MB)\n",
    "NUMBER_OF_TWEETS = 14485\n",
    "\n",
    "# Number of times the dataset is produced by Kafka\n",
    "NUMBER_OF_PRODUCTIONS = 10\n",
    "\n",
    "NUMBER_OF_PRODUCTIONS = 100\n",
    "\n",
    "DATASET_SIZE = 10\n",
    "\n",
    "NUMBER_OF_EXECUTIONS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-fcf4ec488e7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0;34m.\u001b[0m\u001b[0madd_sink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkafka_producer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m '''\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyflink/datastream/stream_execution_environment.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, job_name)\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0mj_stream_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_stream_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear_transformations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mJobExecutionResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_j_stream_execution_environment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj_stream_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexecute_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Flink Streaming Job'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJobClient\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1282\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1286\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "measured_times = []\n",
    "records_received = 0\n",
    "start_time = -1\n",
    "iterations = 0\n",
    "kafka_props = {'bootstrap.servers': 'localhost:9092', 'group.id': 'twitter_consumers'}\n",
    "\n",
    "\n",
    "def collect_stats():\n",
    "    throughput_mbs = [(DATASET_SIZE * NUMBER_OF_PRODUCTIONS) / x for x in measured_times]\n",
    "    throughput_messages = [NUMBER_OF_TWEETS / x for x in measured_times]\n",
    "    min_throughput = min(throughput_mbs)\n",
    "    max_throughput = max(throughput_mbs)\n",
    "    avg_throughput = mean(throughput_mbs)\n",
    "\n",
    "    print(\"============ Throughput Results ============\")\n",
    "    for i in len(throughput_mbs):\n",
    "        print(\"Iteration {]: {} MB/s | {} Msgs/s\".format(str(i), throughput_mbs[i], throughput_messages[i]))\n",
    "\n",
    "    # Plot Max, Min, Avg MB/s\n",
    "    fig = plt.figure(figsize=(10, 6), dpi=80)\n",
    "    plt.bar(0, max_throughput, width=1, color='navy')\n",
    "    plt.bar(1, avg_throughput, width=1, color='darkcyan')\n",
    "    plt.bar(2, min_throughput, width=1, color='skyblue')\n",
    "\n",
    "    plt.tick_params(labelleft=True, labelbottom=False)\n",
    "    plt.legend([\"Maximum MB/s\", \"Average MB/s\", \"Minimum MB/s\"], prop={'size': 12}, bbox_to_anchor=(1.05, 0.8))\n",
    "    plt.title(\"Summarization of Throughput in MB/s\", fontsize=14, pad=12)\n",
    "    plt.savefig('throughput.png')\n",
    "\n",
    "\n",
    "class MyProcessFunction(ProcessFunction):\n",
    "\n",
    "    def process_element(self, value, ctx: 'ProcessFunction.Context'):\n",
    "        # global might not be ideal here (parallelism possible?) but currently nothing else comes to mind\n",
    "        global records_received\n",
    "        global start_time\n",
    "        global iterations\n",
    "        cur_time = time.time()\n",
    "        if records_received == 0:\n",
    "            start_time = cur_time\n",
    "        records_received += 1\n",
    "        latency = (cur_time * 1000) - ctx.timestamp()\n",
    "        result = str(latency)\n",
    "        yield result\n",
    "        if records_received % (NUMBER_OF_TWEETS * NUMBER_OF_PRODUCTIONS) == 0:\n",
    "            end_time = cur_time()\n",
    "            total_time = end_time - start_time\n",
    "            measured_times.append(total_time)\n",
    "            records_received = 0\n",
    "            iterations += 1\n",
    "        if iterations == NUMBER_OF_EXECUTIONS:\n",
    "            collect_stats()\n",
    "\n",
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "'''\n",
    "An execution environment defines a default parallelism for all operators, data sources,\n",
    "and data sinks it executes\n",
    "'''\n",
    "env.set_stream_time_characteristic(TimeCharacteristic.EventTime)\n",
    "\n",
    "# Add the Kafka Connector Dependency\n",
    "env.add_jars(KAFKA_CONNECTOR_JAR)\n",
    "env.add_classpaths(KAFKA_CONNECTOR_JAR)\n",
    "\n",
    "kafka_consumer = FlinkKafkaConsumer(\"twitter-stream\", SimpleStringSchema(), consumer_props)\n",
    "kafka_producer = FlinkKafkaProducer(\"twitter-stream-results\", SimpleStringSchema(), producer_props)\n",
    "\n",
    "stream = env.add_source(kafka_consumer)\n",
    "stream.process(MyProcessFunction(), output_type=Types.STRING()) \\\n",
    "      .add_sink(kafka_producer)\n",
    "\n",
    "env.execute()\n",
    "\n",
    "'''\n",
    "Weird stuff:\n",
    "    - Sometimes more records than emitted by Kafka (sometimes also less)\n",
    "    - print output sometimes was \"Latency: {}   Record-Number: {}   Total Number of Records: {}\".format(str(latency), str(len(latencies)), str(NUMBER_OF_TWEETS * NUMBER_OF_PRODUCTIONS))\n",
    "      and sometimes str(latency)!?!\n",
    "    - what value for parallelism?\n",
    "    \n",
    "Notes:\n",
    "    - Flink parallelism: https://stackoverflow.com/questions/61139187/what-is-the-difference-between-parallelism-and-parallel-computing-in-flink\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Check the results by consuming the result stream from Kafka\n",
    "import platform, socket, json, psutil, logging, multiprocessing\n",
    "\n",
    "from time import time, perf_counter\n",
    "from confluent_kafka import Consumer, KafkaError\n",
    "\n",
    "# Create Kafka Consumer\n",
    "consumer_config = {\n",
    "    \"bootstrap.servers\": \"localhost:9092\",\n",
    "    \"group.id\": \"twitter-consumers\",\n",
    "    \"client.id\": \"client-1\",\n",
    "    \"enable.auto.commit\": True,\n",
    "    \"session.timeout.ms\": 6000,\n",
    "    \"default.topic.config\": {\"auto.offset.reset\": \"smallest\"}\n",
    "}\n",
    "c = Consumer(consumer_config)\n",
    "\n",
    "msg_counter = 0\n",
    "\n",
    "c.subscribe([\"twitter-stream-results\"])\n",
    "while True:             \n",
    "    msg = c.poll(0.5)\n",
    "\n",
    "    if msg is None:\n",
    "        continue\n",
    "    elif not msg.error():\n",
    "        msg_counter += 1\n",
    "        print(\"Message: {} Latency: {}\".format(msg_counter, msg.value()))\n",
    "#         msg_counter += 1\n",
    "        # Start the timer once the first message was received\n",
    "#         if msg_counter == 1:\n",
    "#             start = perf_counter()\n",
    "#         latencies.append(latency)\n",
    "    elif msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "        print(\"End of partition reached {}/{}\".format(msg.topic(), msg.partition()))\n",
    "        print(\"Messages consumed: {}\".format(msg_counter))\n",
    "    else:\n",
    "        print(\"Error occured: {}\".format(msg.error().str()))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(iterations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
