{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Kafka Streaming Analytics\n",
    "In this notebook Apache Kafka is going to be used and analyzed with reference to the streaming performance using the twitter dataset.\n",
    "\n",
    "In this case, we are going to use only one **Kafka Broker** that streams the data to the **Kafka Consumer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting confluent-kafka\n",
      "  Downloading confluent_kafka-1.6.1-cp37-cp37m-manylinux2010_x86_64.whl (2.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.7 MB 6.5 MB/s eta 0:00:01     |██████████████████████████▋     | 2.3 MB 6.5 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: confluent-kafka\n",
      "Successfully installed confluent-kafka-1.6.1\n"
     ]
    }
   ],
   "source": [
    "# Install the Python Client for Apache Kafka\n",
    "!pip install confluent-kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies and set constants\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from confluent_kafka import Producer, Consumer, KafkaError\n",
    "\n",
    "TWITTER_DATA_PATH = \"../data/dataset.json\"\n",
    "KAFKA_TOPIC_TWITTER = \"twitter-stream\"\n",
    "CONSUMER_GROUP_ID = \"twitter-consumers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder: Running Kafka Architecture required\n",
    "The following cells assume a running Apache Kafka Environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce the data / write it to the Kafka Cluster\n",
    "producer_config = {\n",
    "    \"bootstrap.servers\": \"localhost:9092\"\n",
    "}\n",
    "p = Producer(producer_config)\n",
    "\n",
    "with open(TWITTER_DATA_PATH, \"r\") as dataset:\n",
    "    for tweet in dataset:       \n",
    "        p.produce(KAFKA_TOPIC_TWITTER, value=tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consume the data\n",
    "consumer_config = {\n",
    "    \"bootstrap.servers\": \"localhost:9092\",\n",
    "    \"group.id\": CONSUMER_GROUP_ID,\n",
    "    \"client.id\": \"client-1\",\n",
    "    \"enable.auto.commit\": True,\n",
    "    \"session.timeout.ms\": 6000,\n",
    "    \"default.topic.config\": {\"auto.offset.reset\": \"smallest\"}\n",
    "}\n",
    "c = Consumer(consumer_config)\n",
    "\n",
    "c.subscribe([KAFKA_TOPIC_TWITTER])\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        msg = c.poll(0.1)\n",
    "        \n",
    "        if msg is None:\n",
    "            continue\n",
    "        elif not msg.error():\n",
    "            # Display the received tweet\n",
    "            print(json.dumps(msg.value(), indent=4, ensure_ascii=False, sort_keys=True))\n",
    "        elif msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "            print(\"End of partition reached {}/{}\".format(msg.topic(), msg.partition()))\n",
    "        else:\n",
    "            print(\"Error occured: {}\".format(msg.error().str()))\n",
    "except KeyBoardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    c.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
