{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Kafka Streaming Analytics\n",
    "### One Broker Setup\n",
    "<br>\n",
    "<hr>\n",
    "\n",
    "#### Component: Producer\n",
    "In this notebook Apache Kafka is going to be used and analyzed with reference to the streaming performance using the twitter dataset.\n",
    "\n",
    "In this case, we are going to use only one **Kafka Broker** that streams the data to the **Kafka Consumer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Python Client for Apache Kafka\n",
    "!pip install confluent-kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies and set constants\n",
    "from confluent_kafka import Producer\n",
    "from time import time, perf_counter\n",
    "\n",
    "# DATA_GENERATION_IN_MB = 1000 # ~ 1GB\n",
    "DATA_GENERATION_IN_MB = 100 # For Testing\n",
    "DATASET_SIZE_IN_MB = 10\n",
    "\n",
    "TWITTER_DATA_PATH = \"/home/ubuntu/Stream-Analytics/data/dataset.json\"\n",
    "KAFKA_TOPIC_TWITTER = \"twitter-stream\"\n",
    "\n",
    "\n",
    "def producer_stats(start: int, end: int, data_size: int):\n",
    "    \"\"\"\n",
    "    Evaluates the throughput for the producer.\n",
    "    :param start: the UTC time in milliseconds (time since epoch)\n",
    "    :param end: the UTC time in milliseconds (time since epoch)\n",
    "    :param data_size: size of produced data in MB\n",
    "    \"\"\"\n",
    "    delta_seconds = float(end - start)\n",
    "    throughput = data_size / delta_seconds\n",
    "    \n",
    "    print(\"Sent {0} MB in {1} seconds.\".format(data_size, delta_seconds))\n",
    "    print(\"Throughput: ~{0} MB/s | ~{1} MBit/s\".format(int(throughput), int(throughput * 8)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder: Running Kafka Architecture required\n",
    "The following cells assume a running Apache Kafka Environment.\n",
    "<hr>\n",
    "<br>\n",
    "\n",
    "Furthermore, as we are using the **Producer** component here, we have to make sure that the **Consumer** component is already running / expecting streaming data to ensure a performance measurement under realistic circumstances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce the data / write it to the Kafka Cluster\n",
    "producer_config = {\n",
    "    \"bootstrap.servers\": \"localhost:9092\",\n",
    "}\n",
    "p = Producer(producer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is left for testing, for benchmark see below\n",
    "start = perf_counter()\n",
    "\n",
    "# Fill the topic with the specified amount of data\n",
    "generation_steps = int(DATA_GENERATION_IN_MB / DATASET_SIZE_IN_MB)\n",
    "with open(TWITTER_DATA_PATH, \"r\") as dataset:\n",
    "    for step in range(generation_steps):\n",
    "        print(f\"Executing data generation step {step}...\")\n",
    "        dataset.seek(0) # Jump back to first line  \n",
    " \n",
    "        for tweet in dataset:\n",
    "            try:\n",
    "                # print(\"IN QUEUE: {}\".format(len(p)))\n",
    "                p.produce(KAFKA_TOPIC_TWITTER, value=tweet)\n",
    "                p.poll(0)\n",
    "            except BufferError:\n",
    "                print('[INFO] Local producer queue is full (%d messages awaiting delivery): Trying again...\\n' % len(p)) \n",
    "                p.poll(1)\n",
    "        \n",
    "                # Retry sending tweet\n",
    "                p.produce(KAFKA_TOPIC_TWITTER, value=tweet)   \n",
    "p.flush(30)\n",
    "\n",
    "end = perf_counter()\n",
    "print(\"Data generation done!\" + \"\\n\")\n",
    "\n",
    "producer_stats(start, end, DATA_GENERATION_IN_MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(data_vol:int = 1000, exec_count: int = 10):\n",
    "    \"\"\" Executes a throughput benchmark procedure, e.g. the amount of MB per second that was \n",
    "        sent by the producer to the Broker. To get meaningful results, the benchmark procedure \n",
    "        is repeated exec_count times.\n",
    "        Default is ~ 1GB per execution, while executing 10 times.\n",
    "        \n",
    "        :param: data_vol Amount of data to send per execution, given in MB. As the dataset is 10 MB large,\n",
    "                         data_vol % 10 = 0 should be true for the given argument.\n",
    "        :param exec_count: Amount of times to repeat the benchmark procedure\n",
    "    \"\"\"\n",
    "    print(\"Executing throughput benchmark ...\")\n",
    "    \n",
    "    throughputs = []\n",
    "    data_generation_steps = int(data_vol / DATASET_SIZE_IN_MB)\n",
    "    \n",
    "    with open(TWITTER_DATA_PATH, \"r\") as dataset:\n",
    "        for execution in range(exec_count):\n",
    "            start = perf_counter()\n",
    "            \n",
    "            for step in range(data_generation_steps):\n",
    "                dataset.seek(0) # Jump back to first line \n",
    "                \n",
    "                for tweet in dataset:\n",
    "                    try:\n",
    "                        p.produce(KAFKA_TOPIC_TWITTER, value=tweet)\n",
    "                        p.poll(0)\n",
    "                    except BufferError:\n",
    "                        print('[INFO] Local producer queue is full (%d messages awaiting delivery): Trying again...\\n' % len(p)) \n",
    "                        p.poll(5)\n",
    "\n",
    "                        # Retry sending tweet\n",
    "                        p.produce(KAFKA_TOPIC_TWITTER, value=tweet)   \n",
    "                        \n",
    "            end = perf_counter()\n",
    "        \n",
    "            delta_seconds = float(end - start)\n",
    "            throughput = data_vol / delta_seconds\n",
    "            throughputs.append(throughput)\n",
    "\n",
    "    p.flush(30)\n",
    "\n",
    "    print(\"Benchmark procedure finished. Visualizing the results ...\")\n",
    "    #for count, throughput in enumerate(throughputs):\n",
    "        # TODO: plot throughputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing throughput benchmark ...\n",
      "Benchmark procedure finished. Visualizing the results ...\n"
     ]
    }
   ],
   "source": [
    "# Execute the benchmark\n",
    "benchmark(data_vol=10, exec_count=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
