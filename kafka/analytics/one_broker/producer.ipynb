{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Kafka Streaming Analytics\n",
    "### One Broker Setup\n",
    "<br>\n",
    "<hr>\n",
    "\n",
    "#### Component: Producer\n",
    "In this notebook Apache Kafka is going to be used and analyzed with reference to the streaming performance using the twitter dataset.\n",
    "\n",
    "In this case, we are going to use only one **Kafka Broker** that streams the data to the **Kafka Consumer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Python Client for Apache Kafka\n",
    "!pip install confluent-kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies and set constants\n",
    "from confluent_kafka import Producer\n",
    "import time \n",
    "\n",
    "# DATA_GENERATION_IN_MB = 1000 # ~ 1GB\n",
    "DATA_GENERATION_IN_MB = 100\n",
    "DATASET_SIZE_IN_MB = 10\n",
    "\n",
    "TWITTER_DATA_PATH = \"/home/ubuntu/Stream-Analytics/data/dataset.json\"\n",
    "KAFKA_TOPIC_TWITTER = \"twitter-stream\"\n",
    "\n",
    "\n",
    "def producer_stats(start, end, data_size):\n",
    "    \"\"\"\n",
    "    Evaluates the throughput for the producer.\n",
    "    :param start: the UTC time in milliseconds (time since epoch)\n",
    "    :param end: the UTC time in milliseconds (time since epoch)\n",
    "    :param data_size: size of produced data in MB\n",
    "    \"\"\"\n",
    "    delta_seconds = (end - start) / 1000\n",
    "    throughput = data_size / delta_seconds\n",
    "    \n",
    "    print(\"Sent {0} MB in {1} seconds.\".format(data_size, delta_seconds))\n",
    "    print(\"Throughput: ~{0} MB/s | ~{1} MBit/s\".format(int(throughput), int(throughput * 8)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder: Running Kafka Architecture required\n",
    "The following cells assume a running Apache Kafka Environment.\n",
    "<hr>\n",
    "<br>\n",
    "\n",
    "Furthermore, as we are using the **Producer** component here, we have to make sure that the **Consumer** component is already running / expecting streaming data to ensure a performance measurement under realistic circumstances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce the data / write it to the Kafka Cluster\n",
    "producer_config = {\n",
    "    \"bootstrap.servers\": \"localhost:9092\",\n",
    "}\n",
    "p = Producer(producer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing data generation step 0...\n",
      "Executing data generation step 1...\n",
      "Executing data generation step 2...\n",
      "Executing data generation step 3...\n",
      "Executing data generation step 4...\n",
      "Executing data generation step 5...\n",
      "Executing data generation step 6...\n",
      "Executing data generation step 7...\n",
      "[INFO] Local producer queue is full (100000 messages awaiting delivery): Trying again after flushing...\n",
      "\n",
      "Executing data generation step 8...\n",
      "[INFO] Local producer queue is full (100000 messages awaiting delivery): Trying again after flushing...\n",
      "\n",
      "Executing data generation step 9...\n",
      "Data generation done!\n",
      "\n",
      "Sent 100 MB in 1.364 seconds.\n",
      "Throughput: ~73 MB/s | ~586 MBit/s\n"
     ]
    }
   ],
   "source": [
    "start = int(time.time() * 1000)\n",
    "\n",
    "# Fill the topic with the specified amount of data\n",
    "generation_steps = int(DATA_GENERATION_IN_MB / DATASET_SIZE_IN_MB)\n",
    "with open(TWITTER_DATA_PATH, \"r\") as dataset:\n",
    "    for step in range(generation_steps):\n",
    "        print(f\"Executing data generation step {step}...\")\n",
    "        dataset.seek(0) # Jump back to first line  \n",
    " \n",
    "        for tweet in dataset:\n",
    "            try:\n",
    "                # print(\"IN QUEUE: {}\".format(len(p)))\n",
    "                p.produce(KAFKA_TOPIC_TWITTER, value=tweet)\n",
    "                p.poll(0)\n",
    "            except BufferError:\n",
    "                print('[INFO] Local producer queue is full (%d messages awaiting delivery): Trying again after flushing...\\n' % len(p)) \n",
    "                p.poll(1)\n",
    "        \n",
    "                # Retry sending tweet\n",
    "                p.produce(KAFKA_TOPIC_TWITTER, value=tweet)   \n",
    "                \n",
    "p.flush(30)\n",
    "end = int(time.time() * 1000)\n",
    "print(\"Data generation done!\" + \"\\n\")\n",
    "\n",
    "producer_stats(start, end, DATA_GENERATION_IN_MB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
