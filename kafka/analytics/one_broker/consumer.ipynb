{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Kafka Streaming Analytics\n",
    "### One Broker Setup\n",
    "<br>\n",
    "<hr>\n",
    "\n",
    "#### Component: Consumer\n",
    "In this notebook Apache Kafka is going to be used and analyzed with reference to the streaming performance using the twitter dataset.\n",
    "\n",
    "In this case, we are going to use only one **Kafka Broker** that streams the data to the **Kafka Consumer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Python Client for Apache Kafka\n",
    "!pip install confluent-kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies and set constants\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import time\n",
    "\n",
    "from confluent_kafka import Consumer, KafkaError\n",
    "\n",
    "CONSUMER_GROUP_ID = \"twitter-consumers\"\n",
    "KAFKA_TOPIC_TWITTER = \"twitter-stream\"\n",
    "\n",
    "\n",
    "def get_kafka_stats(json_stats_bytes):\n",
    "    \"\"\" Callback for the Apache Kafka Consumer Configuration which \n",
    "    requests performance metrics / statistics from Kafka. \n",
    "    \n",
    "    :param json_stats: The JSON statistics data in bytes\n",
    "    \"\"\"\n",
    "    # Decode the bytes into a python dictionary representing the JSON\n",
    "    stats = json.loads(json_stats_bytes)\n",
    "    # TODO: Check if we can actually use those statistics. If so, uncomment them in the consumer config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder: Running Kafka Architecture required\n",
    "The following cells assume a running Apache Kafka Environment.\n",
    "<hr>\n",
    "<br>\n",
    "\n",
    "Furthermore, as we are using the **Consumer** component here, we have to make sure to start the consumer first before we start the Producer to ensure a performance measurement under realistic circumstances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consume the data\n",
    "consumer_config = {\n",
    "    \"bootstrap.servers\": \"localhost:9092\",\n",
    "    \"group.id\": CONSUMER_GROUP_ID,\n",
    "    \"client.id\": \"client-1\",\n",
    "#     \"stats_cb\": get_kafka_stats,\n",
    "#     \"statistics.interval.ms\": 20,\n",
    "#     'api.version.request': True,\n",
    "    \"enable.auto.commit\": True,\n",
    "    \"session.timeout.ms\": 6000,\n",
    "    \"default.topic.config\": {\"auto.offset.reset\": \"smallest\"}\n",
    "}\n",
    "c = Consumer(consumer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KeyBoardInterrupt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6f2697a419b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6f2697a419b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error occured: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mexcept\u001b[0m \u001b[0mKeyBoardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KeyBoardInterrupt' is not defined"
     ]
    }
   ],
   "source": [
    "c.subscribe([KAFKA_TOPIC_TWITTER])\n",
    "\n",
    "msg_count = 0\n",
    "try:\n",
    "    while True:\n",
    "            msg = c.poll(0)\n",
    "            \n",
    "            if msg is None:\n",
    "                continue\n",
    "            elif not msg.error():\n",
    "                timestamp_type, timestamp = msg.timestamp()\n",
    "                delta = (time.time() * 1000) - timestamp\n",
    "\n",
    "                msg_count += 1\n",
    "                print(\"Message {} received | Timestamp: {} | Latency: {}\".format(msg_count, msg.timestamp()[1], delta))\n",
    "\n",
    "                # Display the received tweet\n",
    "                #tweet_json = json.loads(msg.value())\n",
    "                #print(json.dumps(tweet_json, indent=4, ensure_ascii=False, sort_keys=True))\n",
    "            elif msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                print(\"End of partition reached {}/{}\".format(msg.topic(), msg.partition()))\n",
    "            else:\n",
    "                print(\"Error occured: {}\".format(msg.error().str()))\n",
    "except KeyBoardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    c.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
